{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downstream task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import time\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size, stride=1, padding=0)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size, stride=1, padding=0)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        return x\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv_block1 = ConvBlock(1, 32, 8)\n",
    "        self.conv_block2 = ConvBlock(32, 64, 4)\n",
    "        self.conv_block3 = ConvBlock(64, 128, 2)\n",
    "        \n",
    "        self.max_pooling = nn.MaxPool1d(kernel_size=4, stride=2)\n",
    "        self.global_max_pooling = nn.AdaptiveMaxPool1d(1)\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_block1(x)\n",
    "        x = self.max_pooling(x)\n",
    "        \n",
    "        x = self.conv_block2(x)\n",
    "        x = self.max_pooling(x)\n",
    "\n",
    "        x = self.conv_block3(x)\n",
    "        x = self.global_max_pooling(x)\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class DownstreamEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DownstreamEncoder, self).__init__()\n",
    "        self.encoder = Encoder()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "premodel_state_paths = sorted([sensor for sensor in Path('pretrained_models').iterdir()], key=lambda s: s.stem)\n",
    "\n",
    "premodel_states = []\n",
    "for premodel_state_path in premodel_state_paths:\n",
    "    premodel_state = torch.load(premodel_state_path)\n",
    "\n",
    "    layers_to_remove = [name for name in premodel_state.keys() if 'projection' in name]\n",
    "    \n",
    "    for name in layers_to_remove:\n",
    "        del premodel_state[name]\n",
    "    \n",
    "    premodel_states.append(premodel_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoders = [DownstreamEncoder()] * len(premodel_states)\n",
    "for i,encoder in enumerate(encoders):\n",
    "    encoder.load_state_dict(premodel_states[i])\n",
    "    for param in encoder.parameters():\n",
    "        param.requires_grad = False\n",
    "    encoder = encoder.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(len(encoders))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownstreamModel(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, num_sensors) -> None:\n",
    "        super(DownstreamModel, self).__init__()\n",
    "\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads, batch_first=True)\n",
    "        self.classifier = nn.Linear(num_sensors*embed_dim, 1)\n",
    "    \n",
    "    def forward(self, features):\n",
    "        x, _ = self.attention(features, features, features)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DownstreamModel(embed_dim=128, num_heads=8, num_sensors=2)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalDataset(Dataset):\n",
    "    def __init__(self, path:Path, sensor_names:list[str]):\n",
    "        self.path = path\n",
    "        self.sensor_name = sensor_names\n",
    "        self.n_sensors = len(sensor_names)\n",
    "        df = pd.read_csv(self.path, index_col='timestamp')\n",
    "        self.input_sequences = df.loc[:,sensor_names]\n",
    "        self.labels = df.loc[:, 'label']\n",
    "\n",
    "\n",
    "    def __getitem__(self, index) -> tuple[torch.Tensor, float]:\n",
    "        sequence_index = self.input_sequences.index.unique()\n",
    "        original_sequence = self.input_sequences.loc[sequence_index[index]]\n",
    "        label = self.labels.loc[sequence_index[index]]\n",
    "        return torch.Tensor(original_sequence.to_numpy().reshape(self.n_sensors, -1)), label.iloc[0]\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_sequences.index.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "sensors = sorted(['AML','EDA'])\n",
    "dataset = MultimodalDataset(Path('Intermediate/proc/labeled_joined/P01_labeled.csv'), sensors)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "participant_labeled_data = [labeled_path for labeled_path in Path('Intermediate/proc/labeled_joined').iterdir()]\n",
    "participants_num = len(participant_labeled_data)\n",
    "random.shuffle(participant_labeled_data)\n",
    "\n",
    "train_datasets = [MultimodalDataset(path, sensors) for path in participant_labeled_data[:int(0.8*participants_num)]]\n",
    "train_dataloader = [DataLoader(dataset, batch_size, shuffle=True) for dataset in train_datasets]\n",
    "\n",
    "val_datasets = [MultimodalDataset(path, sensors) for path in participant_labeled_data[int(0.8*participants_num):int(0.9*participants_num)]]\n",
    "val_dataloaders = [DataLoader(dataset, batch_size, shuffle=False) for dataset in val_datasets]\n",
    "\n",
    "test_datasets = [MultimodalDataset(path, sensors) for path in participant_labeled_data[int(0.9*participants_num):]]\n",
    "test_dataloaders = [DataLoader(dataset, batch_size, shuffle=False) for dataset in test_datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.0000e-02.\n"
     ]
    }
   ],
   "source": [
    "tr_ep_loss = []\n",
    "tr_ep_acc = []\n",
    "\n",
    "val_ep_loss = []\n",
    "val_ep_acc = []\n",
    "\n",
    "min_val_loss = 100.0\n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "dsoptimizer = torch.optim.SGD(model.parameters(),lr = 0.01, momentum = 0.9)\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(dsoptimizer, step_size=1, gamma=0.98, last_epoch=-1, verbose = True)\n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============== Epoch :   1 ===============\n",
      "ESTIMATING TRAINING METRICS.............\n",
      "TRAINING BINARY CROSSENTROPY LOSS:  0.6435069840800101\n",
      "TRAINING BINARY ACCURACY:  0.6691887347625053\n",
      "ESTIMATING VALIDATION METRICS.............\n",
      "VALIDATION BINARY CROSSENTROPY LOSS:  0.5759311267930418\n",
      "VALIDATION BINARY ACCURACY:  0.7332480409403487\n",
      "Adjusting learning rate of group 0 to 9.8000e-03.\n",
      "Saving model...\n",
      "Time Taken : 6.05 minutes\n",
      "=============== Epoch :   2 ===============\n",
      "ESTIMATING TRAINING METRICS.............\n",
      "TRAINING BINARY CROSSENTROPY LOSS:  0.64512957050134\n",
      "TRAINING BINARY ACCURACY:  0.6691887347625053\n",
      "ESTIMATING VALIDATION METRICS.............\n",
      "VALIDATION BINARY CROSSENTROPY LOSS:  0.5760034060621088\n",
      "VALIDATION BINARY ACCURACY:  0.7332480409403487\n",
      "Adjusting learning rate of group 0 to 9.6040e-03.\n",
      "Time Taken : 6.00 minutes\n",
      "=============== Epoch :   3 ===============\n",
      "ESTIMATING TRAINING METRICS.............\n",
      "TRAINING BINARY CROSSENTROPY LOSS:  0.6440213805947722\n",
      "TRAINING BINARY ACCURACY:  0.6691887347625053\n",
      "ESTIMATING VALIDATION METRICS.............\n",
      "VALIDATION BINARY CROSSENTROPY LOSS:  0.5761028517093927\n",
      "VALIDATION BINARY ACCURACY:  0.7332480409403487\n",
      "Adjusting learning rate of group 0 to 9.4119e-03.\n",
      "Time Taken : 6.00 minutes\n",
      "=============== Epoch :   4 ===============\n",
      "ESTIMATING TRAINING METRICS.............\n",
      "TRAINING BINARY CROSSENTROPY LOSS:  0.6453421360405146\n",
      "TRAINING BINARY ACCURACY:  0.6691887347625053\n",
      "ESTIMATING VALIDATION METRICS.............\n",
      "VALIDATION BINARY CROSSENTROPY LOSS:  0.576420810232798\n",
      "VALIDATION BINARY ACCURACY:  0.7332480409403487\n",
      "Adjusting learning rate of group 0 to 9.2237e-03.\n",
      "Time Taken : 6.00 minutes\n",
      "=============== Epoch :   5 ===============\n",
      "ESTIMATING TRAINING METRICS.............\n",
      "TRAINING BINARY CROSSENTROPY LOSS:  0.6443216060500103\n",
      "TRAINING BINARY ACCURACY:  0.6691887347625053\n",
      "ESTIMATING VALIDATION METRICS.............\n",
      "VALIDATION BINARY CROSSENTROPY LOSS:  0.5765672961862417\n",
      "VALIDATION BINARY ACCURACY:  0.7332480409403487\n",
      "Adjusting learning rate of group 0 to 9.0392e-03.\n",
      "Time Taken : 6.02 minutes\n",
      "=============== Epoch :   6 ===============\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[146], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m dsoptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataloader \u001b[38;5;129;01min\u001b[39;00m train_dataloader:\n\u001b[0;32m---> 15\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x,y \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m     16\u001b[0m         x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m     17\u001b[0m         y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcuda()\n",
      "File \u001b[0;32m~/miniconda3/envs/anas/lib/python3.9/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/anas/lib/python3.9/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/anas/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/anas/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[123], line 12\u001b[0m, in \u001b[0;36mMultimodalDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;28mfloat\u001b[39m]:\n\u001b[0;32m---> 12\u001b[0m     sequence_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_sequences\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     original_sequence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_sequences\u001b[38;5;241m.\u001b[39mloc[sequence_index[index]]\n\u001b[1;32m     14\u001b[0m     label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels\u001b[38;5;241m.\u001b[39mloc[sequence_index[index]]\n",
      "File \u001b[0;32m~/miniconda3/envs/anas/lib/python3.9/site-packages/pandas/core/indexes/base.py:3047\u001b[0m, in \u001b[0;36mIndex.unique\u001b[0;34m(self, level)\u001b[0m\n\u001b[1;32m   3044\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_unique:\n\u001b[1;32m   3045\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_view()\n\u001b[0;32m-> 3047\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3048\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shallow_copy(result)\n",
      "File \u001b[0;32m~/miniconda3/envs/anas/lib/python3.9/site-packages/pandas/core/base.py:1025\u001b[0m, in \u001b[0;36mIndexOpsMixin.unique\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m     result \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39munique()\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1025\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda3/envs/anas/lib/python3.9/site-packages/pandas/core/algorithms.py:401\u001b[0m, in \u001b[0;36munique\u001b[0;34m(values)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21munique\u001b[39m(values):\n\u001b[1;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;124;03m    Return unique values based on a hash table.\u001b[39;00m\n\u001b[1;32m    310\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;124;03m    array([('a', 'b'), ('b', 'a'), ('a', 'c')], dtype=object)\u001b[39;00m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 401\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43munique_with_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/anas/lib/python3.9/site-packages/pandas/core/algorithms.py:440\u001b[0m, in \u001b[0;36munique_with_mask\u001b[0;34m(values, mask)\u001b[0m\n\u001b[1;32m    438\u001b[0m table \u001b[38;5;241m=\u001b[39m hashtable(\u001b[38;5;28mlen\u001b[39m(values))\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 440\u001b[0m     uniques \u001b[38;5;241m=\u001b[39m \u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    441\u001b[0m     uniques \u001b[38;5;241m=\u001b[39m _reconstruct_data(uniques, original\u001b[38;5;241m.\u001b[39mdtype, original)\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m uniques\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    stime = time.time()\n",
    "    print(\"=============== Epoch : %3d ===============\"%(epoch+1))\n",
    "    \n",
    "    loss_sublist = np.array([])\n",
    "    acc_sublist = np.array([])\n",
    "    \n",
    "    #iter_num = 0\n",
    "    model.train()\n",
    "    \n",
    "    dsoptimizer.zero_grad()\n",
    "    \n",
    "    for dataloader in train_dataloader:\n",
    "        for x,y in dataloader:\n",
    "            x = x.cuda()\n",
    "            y = y.unsqueeze(1).cuda()\n",
    "            \n",
    "            features = [encoder(x[:,i,:].unsqueeze(1)).unsqueeze(1) for i,encoder in enumerate(encoders)]\n",
    "        \n",
    "            features = torch.concat(features, dim=1)\n",
    "\n",
    "            z = model(features)\n",
    "            \n",
    "            dsoptimizer.zero_grad()\n",
    "            \n",
    "            tr_loss = loss_fn(z,y)\n",
    "            tr_loss.backward()\n",
    "\n",
    "            preds = torch.exp(z.cpu().data)/torch.sum(torch.exp(z.cpu().data))\n",
    "            \n",
    "            dsoptimizer.step()\n",
    "            \n",
    "            loss_sublist = np.append(loss_sublist, tr_loss.cpu().data)\n",
    "            acc_sublist = np.append(acc_sublist,np.array(np.argmax(preds,axis=1)==y.cpu().data.view(-1)).astype('int'),axis=0)\n",
    "        \n",
    "    print('ESTIMATING TRAINING METRICS.............')\n",
    "    \n",
    "    print('TRAINING BINARY CROSSENTROPY LOSS: ',np.mean(loss_sublist))\n",
    "    print('TRAINING BINARY ACCURACY: ',np.mean(acc_sublist))\n",
    "    \n",
    "    tr_ep_loss.append(np.mean(loss_sublist))\n",
    "    tr_ep_acc.append(np.mean(acc_sublist))\n",
    "    \n",
    "    print('ESTIMATING VALIDATION METRICS.............')\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    loss_sublist = np.array([])\n",
    "    acc_sublist = np.array([])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for dataloader in val_dataloaders:\n",
    "            for x,y in dataloader:\n",
    "                x = x.cuda()\n",
    "                y = y.unsqueeze(1).cuda()\n",
    "                \n",
    "                features = [encoder(x[:,i,:].unsqueeze(1)).unsqueeze(1) for i,encoder in enumerate(encoders)]\n",
    "        \n",
    "                features = torch.concat(features, dim=1)\n",
    "\n",
    "                z = model(features)\n",
    "\n",
    "                val_loss = loss_fn(z,y)\n",
    "\n",
    "                preds = torch.exp(z.cpu().data)/torch.sum(torch.exp(z.cpu().data))\n",
    "\n",
    "                loss_sublist = np.append(loss_sublist, val_loss.cpu().data)\n",
    "                acc_sublist = np.append(acc_sublist,np.array(np.argmax(preds,axis=1)==y.cpu().data.view(-1)).astype('int'),axis=0)\n",
    "    \n",
    "    print('VALIDATION BINARY CROSSENTROPY LOSS: ',np.mean(loss_sublist))\n",
    "    print('VALIDATION BINARY ACCURACY: ',np.mean(acc_sublist))\n",
    "    \n",
    "    val_ep_loss.append(np.mean(loss_sublist))\n",
    "    val_ep_acc.append(np.mean(acc_sublist))\n",
    "    \n",
    "    lr_scheduler.step()\n",
    "    \n",
    "    if np.mean(loss_sublist) <= min_val_loss:\n",
    "        min_val_loss = np.mean(loss_sublist) \n",
    "        print('Saving model...')\n",
    "        torch.save(model.state_dict(), 'downstream_models/downstream.pt')\n",
    "    \n",
    "    print(\"Time Taken : %.2f minutes\"%((time.time()-stime)/60.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST BINARY CROSSENTROPY LOSS:  0.7747196915014206\n",
      "TEST BINARY ACCURACY:  0.5250884622077847\n"
     ]
    }
   ],
   "source": [
    "model = DownstreamModel(128, 8 ,2).cuda()\n",
    "model.load_state_dict(torch.load('downstream_models/downstream.pt'))\n",
    "\n",
    "model.eval()\n",
    "    \n",
    "loss_sublist = np.array([])\n",
    "acc_sublist = np.array([])\n",
    "\n",
    "with torch.no_grad():\n",
    "    for dataloader in test_dataloaders:\n",
    "        for x,y in dataloader:\n",
    "            x = x.cuda()\n",
    "            y = y.unsqueeze(1).cuda()\n",
    "            \n",
    "            features = [encoder(x[:,i,:].unsqueeze(1)).unsqueeze(1) for i,encoder in enumerate(encoders)]\n",
    "        \n",
    "            features = torch.concat(features, dim=1)\n",
    "\n",
    "            z = model(features)\n",
    "\n",
    "            val_loss = loss_fn(z,y)\n",
    "\n",
    "            preds = torch.exp(z.cpu().data)/torch.sum(torch.exp(z.cpu().data))\n",
    "\n",
    "            loss_sublist = np.append(loss_sublist, val_loss.cpu().data)\n",
    "            acc_sublist = np.append(acc_sublist,np.array(np.argmax(preds,axis=1)==y.cpu().data.view(-1)).astype('int'),axis=0)\n",
    "\n",
    "print('TEST BINARY CROSSENTROPY LOSS: ',np.mean(loss_sublist))\n",
    "print('TEST BINARY ACCURACY: ',np.mean(acc_sublist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
